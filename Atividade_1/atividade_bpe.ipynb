{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade 1: Byte Pair Encoding\n",
    "Aluno: Rennê Ruan Alves Oliveira - Matrícula 242101209 - PPGI\n",
    "\n",
    "Esta atividade tem como objetivo a reprodução do algoritmo *Byte Pair Encoding* (BPE). O algoritmo BPE é utilizado na área de Processamento de Linguagem Natural para representar um grande vocabulário em um conjunto de trechos de palavras.\n",
    "\n",
    "O algoritmo original segue os seguintes passsos:\n",
    "- Dado um texto de entrada consideramos cada caractere do mesmo como uma unidade de token\n",
    "- Verificamos então todas as combinações de adjacências no texto e computamos as frequências de adjacências iguais\n",
    "- Mesclamos os caracteres que apresentam maior frequência de adjacência e consideramos essa mescla como um novo token no vocabulário\n",
    "- Iteramos e repetimos o processo, até o critério estabelecido\n",
    "\n",
    "Para este projeto foi utilizado um ambiente de execução Python 3.13 criado a partir do gerenciador Anaconda. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação do corpo de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos aplicar o corpo de texto importado a nossa classe `BPETokenizer` criada no arquivo `tokenizer.py`, treinando o tokenizador com o texto fornecido e criando um vocabulário de tokens.\n",
    "Esta classe utiliza dos métodos `count_pairs` e `merge_tokens` desenvolvidas no arquivo `helper_functions.py`. Estes auxiliam no processo de contagem de adjacências e junção de tokens respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('l', 'o'): 1, ('o', 'w'): 3, ('w', ','): 2, (',', ' '): 2, (' ', 'n'): 1, ('n', 'o'): 1, (' ', 'b'): 1, ('b', 'o'): 1}\n",
      "['l', 'Z', ',', ' ', 'n', 'Z', ',', ' ', 'b', 'Z']\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import count_pairs, merge_tokens\n",
    "\n",
    "# Exemplificação do uso das funções\n",
    "text_list = list(\"low, now, bow\")\n",
    "print(count_pairs(text_list))\n",
    "# Temos que a sequência ow é a que mais se repete entre os atuais tokens do texto.\n",
    "\n",
    "# Temos como parâmetros a lista de tokens, a tupla com maior repetição\n",
    "# E por fim como iremos chamar o novo token correspondente a tupla\n",
    "print(merge_tokens(text_list, (\"o\", \"w\"), \"Z\"))\n",
    "\n",
    "# Esperamos que de resultado ow seja substituído por Z em nosso texto de exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `BPETokenizer` utiliza das funções acima iterando e criando um vocabulário, com os tokens previamente existentes e os gerados pelas junções, e um mapeamento de junções. Este vocabulário e mapeamento será necessário para realizarmos o processo de codificação e decodificação de um texto novo que seja fornecido após o processo de treinamento.\n",
    "\n",
    "O tokenizador aqui criado tem como vocabulário inicial uma sequência de 256 tokens, representando as possibilidades de caractere em 1 byte a partir da codificação UTF-8. As junções então criadas serão postas após os 256 tokens originais. Isto também é importante para definirmos o critério de parada, ao declararmos nosso `BPETokenizer` devemos informar quantos laços de junção gostaríamos de realizar, ou seja a quantia de tokens final será 256 + [quantia de iterações]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto recebido em caracteres: 301\n",
      "Tamanho do texto recebido em bytes: 301\n",
      "Tamanho da lista de tokens após BPE: 213\n",
      "Taxa de compressão (Tokens originais/Tokens BPE): 1.41X\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import BPETokenizer\n",
    "\n",
    "num_merges = 20\n",
    "\n",
    "bpe = BPETokenizer(num_merges)\n",
    "text = \"The conda environments are prepended to your PATH variable, so when you are trying to run the executable 'ipython', Linux will not find 'ipython' in your activated environment (since it doesn't exist there), but it will continue searching for it, and eventually find it wherever you have it installed.\"\n",
    "\n",
    "bpe.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe.save(\"vocabula\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_activities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
