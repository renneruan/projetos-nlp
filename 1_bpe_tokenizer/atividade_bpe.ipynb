{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atividade 1: Byte Pair Encoding\n",
    "\n",
    "Esta atividade tem como objetivo a reprodução do algoritmo *Byte Pair Encoding* (BPE). O algoritmo BPE é utilizado na área de Processamento de Linguagem Natural para representar um grande vocabulário em um conjunto de trechos de palavras.\n",
    "\n",
    "O algoritmo original segue os seguintes passsos:\n",
    "- Dado um texto de entrada consideramos cada caractere do mesmo como uma unidade de token\n",
    "- Verificamos então todas as combinações de adjacências no texto e computamos as frequências de adjacências iguais\n",
    "- Mesclamos os caracteres que apresentam maior frequência de adjacência e consideramos essa mescla como um novo token no vocabulário\n",
    "- Iteramos e repetimos o processo, até o critério estabelecido\n",
    "\n",
    "Para este projeto foi utilizado um ambiente de execução Python 3.13 criado a partir do gerenciador Anaconda.\n",
    "\n",
    "Se torna necessário também que o corpo disponibilizado esteja no mesmo diretório que este notebook na pasta corpus/ contendo os arquivos JSONs a serem analisados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos lidos: 10000\n"
     ]
    }
   ],
   "source": [
    "# Aqui assumimos que o diretório com o corpo de arquivos JSON está no mesmo diretório que este notebook\n",
    "directory = \"corpus/\"\n",
    "count = 0\n",
    "\n",
    "merged_text = (\n",
    "    io.StringIO()\n",
    ")  # Utilizando StringIO para melhorar a eficiência da concatenação\n",
    "# Importação do corpo de texto em lote de acordo com o diretório\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            count = count + 1\n",
    "            # corpus_text = corpus_text + \" \" + data[\"text\"] -> Resultado da leitura em mais de 3 minutos\n",
    "            merged_text.write(\n",
    "                data[\"text\"] + \" \"\n",
    "            )  # -> Leitura em 1:30 min e posteriormente mantém arquivos em memória\n",
    "\n",
    "print(f\"Arquivos lidos: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho total do texto lido: 71033261\n"
     ]
    }
   ],
   "source": [
    "corpus = merged_text.getvalue()\n",
    "\n",
    "print(f\"Tamanho total do texto lido: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos aplicar o corpo de texto importado a nossa classe `BPETokenizer` criada no arquivo `tokenizer.py`, treinando o tokenizador com o texto fornecido e criando um vocabulário de tokens.\n",
    "Esta classe utiliza dos métodos `count_pairs` e `merge_tokens` desenvolvidas no arquivo `helper_functions.py`. Estes auxiliam no processo de contagem de adjacências e junção de tokens respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import count_pairs, merge_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('l', 'o'): 1, ('o', 'w'): 3, ('w', ','): 2, (',', ' '): 2, (' ', 'n'): 1, ('n', 'o'): 1, (' ', 'b'): 1, ('b', 'o'): 1}\n",
      "['l', 'Z', ',', ' ', 'n', 'Z', ',', ' ', 'b', 'Z']\n"
     ]
    }
   ],
   "source": [
    "# Exemplificação do uso das funções\n",
    "text_list = list(\"low, now, bow\")\n",
    "print(count_pairs(text_list))\n",
    "# A sequência ow é a que mais se repete entre os atuais tokens do texto.\n",
    "\n",
    "# Temos como parâmetros: a lista de tokens, a tupla com maior repetição\n",
    "# E por fim como iremos chamar o novo token correspondente a tupla\n",
    "print(merge_tokens(text_list, (\"o\", \"w\"), \"Z\"))\n",
    "\n",
    "# Esperamos que de resultado ow seja substituído por Z em nosso texto de exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe `BPETokenizer` utiliza das funções acima iterando e criando um vocabulário, com os tokens previamente existentes e os gerados pelas junções, além de registrar o mapeamento de junções. Este vocabulário e mapeamento serão necessários para realizarmos a codificação e decodificação de um novo text que seja fornecido após o treinamento.\n",
    "\n",
    "O tokenizador aqui criado tem como vocabulário inicial uma sequência de 256 tokens, representando as possibilidades de caractere em 1 byte a partir da codificação UTF-8. As junções então criadas serão postas após os 256 tokens originais. Isto também é importante para definirmos o critério de parada, ao declararmos nosso `BPETokenizer` devemos informar quantos laços de junção gostaríamos de realizar, ou seja a quantia de tokens final será 256 + [quantia de iterações].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import BPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATENÇÃO: A execução da próxima célula demanda um tempo de processamento considerável, em ambiente local teve um tempo de execução aproximado de 10 minutos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto recebido em caracteres: 71033261\n",
      "Tamanho do texto recebido em bytes: 73037124\n",
      "Tamanho da lista de tokens após BPE: 57540928\n",
      "Taxa de compressão (Tokens originais/Tokens BPE): 1.27X\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer com 276 tokens gerados.\n",
    "num_merges = 20\n",
    "\n",
    "bpe = BPETokenizer(num_merges)\n",
    "\n",
    "bpe.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O treinamento do tokenizador é passível de otimização, por meio de paralelização ou segmentação de chunks, uma vez que leva um tempo considerável para processar os milhões de caracteres inseridos. Para o propósito deste trabalho de manter o código simplificado, estas otimizações não foram realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: [o][ ] -> [o ]\n",
      "257: [a][ ] -> [a ]\n",
      "258: [e][ ] -> [e ]\n",
      "259: [s][ ] -> [s ]\n",
      "260: [,][ ] -> [, ]\n",
      "261: [d][e ] -> [de ]\n",
      "262: [e][n] -> [en]\n",
      "263: [m][ ] -> [m ]\n",
      "264: [o][r] -> [or]\n",
      "265: [e][r] -> [er]\n",
      "266: [a][n] -> [an]\n",
      "267: [a][r] -> [ar]\n",
      "268: [e][s] -> [es]\n",
      "269: [c][o] -> [co]\n",
      "270: [.][ ] -> [. ]\n",
      "271: [d][o ] -> [do ]\n",
      "272: [o][s ] -> [os ]\n",
      "273: [i][n] -> [in]\n",
      "274: [a][l] -> [al]\n",
      "275: [a][s ] -> [as ]\n"
     ]
    }
   ],
   "source": [
    "# Salvamos os tokens gerados, temos que para o grande corpo de texto recebido, foi necessário 10 minutos em máquina local para processar 20 tokens.\n",
    "bpe.save(\"voc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 268, 116, 258, 261, 269, 100, 105, 102, 105, 99, 97, 195, 167, 195, 163, 256, 112, 267, 257, 66, 80, 69, 32, 269, 263, 50, 55, 54, 32, 116, 111, 107, 262, 115]\n",
      "Teste de codificação para BPE com 276 tokens\n"
     ]
    }
   ],
   "source": [
    "t = bpe.encode(\"Teste de codificação para BPE com 276 tokens\")\n",
    "print(t)\n",
    "\n",
    "d = bpe.decode(t)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exemplificar a geração de um Tokenizador com mais tokens, vamos utilizar apenas os 100 primeiros arquivos JSON do corpo fornecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_text_100 = io.StringIO()\n",
    "\n",
    "json_list = os.listdir(directory)\n",
    "for filename in json_list[:100]:\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "            merged_text_100.write(data[\"text\"] + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do texto recebido em caracteres: 486034\n",
      "Tamanho do texto recebido em bytes: 498597\n",
      "Tamanho da lista de tokens após BPE: 254527\n",
      "Taxa de compressão (Tokens originais/Tokens BPE): 1.96X\n"
     ]
    }
   ],
   "source": [
    "# Tokenizador com 512 tokens\n",
    "\n",
    "num_merges = 512 - 256\n",
    "bpe512 = BPETokenizer(num_merges)\n",
    "\n",
    "corpus_100 = merged_text_100.getvalue()\n",
    "bpe512.train(corpus_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: [o][ ] -> [o ]\n",
      "257: [a][ ] -> [a ]\n",
      "258: [e][ ] -> [e ]\n",
      "259: [s][ ] -> [s ]\n",
      "260: [,][ ] -> [, ]\n",
      "261: [d][e ] -> [de ]\n",
      "262: [e][r] -> [er]\n",
      "263: [a][n] -> [an]\n",
      "264: [e][n] -> [en]\n",
      "265: [m][ ] -> [m ]\n",
      "266: [o][r] -> [or]\n",
      "267: [a][r] -> [ar]\n",
      "268: [d][o ] -> [do ]\n",
      "269: [e][s] -> [es]\n",
      "270: [.][ ] -> [. ]\n",
      "271: [o][s ] -> [os ]\n",
      "272: [i][n] -> [in]\n",
      "273: [a][l] -> [al]\n",
      "274: [o][n] -> [on]\n",
      "275: [a][s ] -> [as ]\n",
      "276: [�][�] -> [ã]\n",
      "277: [d][a ] -> [da ]\n",
      "278: [i][c] -> [ic]\n",
      "279: [en][t] -> [ent]\n",
      "280: [s][t] -> [st]\n",
      "281: [ã][o ] -> [ão ]\n",
      "282: [�][�] -> [ç]\n",
      "283: [a][t] -> [at]\n",
      "284: [q][u] -> [qu]\n",
      "285: [r][i] -> [ri]\n",
      "286: [r][e] -> [re]\n",
      "287: [�][�] -> [é]\n",
      "288: [a][d] -> [ad]\n",
      "289: [c][i] -> [ci]\n",
      "290: [c][o] -> [co]\n",
      "291: [e][l] -> [el]\n",
      "292: [e][m ] -> [em ]\n",
      "293: [e][s ] -> [es ]\n",
      "294: [a][s] -> [as]\n",
      "295: [a][m] -> [am]\n",
      "296: [u][ ] -> [u ]\n",
      "297: [�][�] -> [í]\n",
      "298: [i][t] -> [it]\n",
      "299: [r][o] -> [ro]\n",
      "300: [a][i] -> [ai]\n",
      "301: [�][�] -> [á]\n",
      "302: [=][=] -> [==]\n",
      "303: [d][i] -> [di]\n",
      "304: [o][s] -> [os]\n",
      "305: [*][ ] -> [* ]\n",
      "306: [i][r] -> [ir]\n",
      "307: [al][ ] -> [al ]\n",
      "308: [a][ç] -> [aç]\n",
      "309: [o ][de ] -> [o de ]\n",
      "310: [o][l] -> [ol]\n",
      "311: [c][on] -> [con]\n",
      "312: [qu][e ] -> [que ]\n",
      "313: [1][9] -> [19]\n",
      "314: [s][e] -> [se]\n",
      "315: [an][t] -> [ant]\n",
      "316: [p][ar] -> [par]\n",
      "317: [u][r] -> [ur]\n",
      "318: [u][n] -> [un]\n",
      "319: [i][l] -> [il]\n",
      "320: [i][d] -> [id]\n",
      "321: [es][t] -> [est]\n",
      "322: [u][l] -> [ul]\n",
      "323: [�][�] -> [ó]\n",
      "324: [u][m] -> [um]\n",
      "325: [a][, ] -> [a, ]\n",
      "326: [e][m] -> [em]\n",
      "327: [or][ ] -> [or ]\n",
      "328: [2][0] -> [20]\n",
      "329: [i][v] -> [iv]\n",
      "330: [i][a ] -> [ia ]\n",
      "331: [o][, ] -> [o, ]\n",
      "332: [)][ ] -> [) ]\n",
      "333: [a][do ] -> [ado ]\n",
      "334: [e][t] -> [et]\n",
      "335: [i][s] -> [is]\n",
      "336: [u][m ] -> [um ]\n",
      "337: [co][m] -> [com]\n",
      "338: [i][ ] -> [i ]\n",
      "339: [i][st] -> [ist]\n",
      "340: [r][a] -> [ra]\n",
      "341: [u][t] -> [ut]\n",
      "342: [o][m] -> [om]\n",
      "343: [er][ ] -> [er ]\n",
      "344: [é][ ] -> [é ]\n",
      "345: [i][m] -> [im]\n",
      "346: [aç][ão ] -> [ação ]\n",
      "347: [e][g] -> [eg]\n",
      "348: [o][u ] -> [ou ]\n",
      "349: [g][u] -> [gu]\n",
      "350: [==][ ] -> [== ]\n",
      "351: [a ][de ] -> [a de ]\n",
      "352: [i][a] -> [ia]\n",
      "353: [d][e] -> [de]\n",
      "354: [ent][e ] -> [ente ]\n",
      "355: [s][u] -> [su]\n",
      "356: [ai][s ] -> [ais ]\n",
      "357: [n][o ] -> [no ]\n",
      "358: [s][e ] -> [se ]\n",
      "359: [co][m ] -> [com ]\n",
      "360: [ar][ ] -> [ar ]\n",
      "361: [or][t] -> [ort]\n",
      "362: [p][o] -> [po]\n",
      "363: [n][a ] -> [na ]\n",
      "364: [a][c] -> [ac]\n",
      "365: [ ][de ] -> [ de ]\n",
      "366: [e][ir] -> [eir]\n",
      "367: [um][a ] -> [uma ]\n",
      "368: [i][g] -> [ig]\n",
      "369: [a][b] -> [ab]\n",
      "370: [par][a ] -> [para ]\n",
      "371: [�][�] -> [ê]\n",
      "372: [d][os ] -> [dos ]\n",
      "373: [t][er] -> [ter]\n",
      "374: [p][r] -> [pr]\n",
      "375: [r][an] -> [ran]\n",
      "376: [a][m ] -> [am ]\n",
      "377: [a][da ] -> [ada ]\n",
      "378: [e][x] -> [ex]\n",
      "379: [a][v] -> [av]\n",
      "380: [:][ ] -> [: ]\n",
      "381: [e][, ] -> [e, ]\n",
      "382: [p][er] -> [per]\n",
      "383: [a][de ] -> [ade ]\n",
      "384: [ ][C] -> [ C]\n",
      "385: [f][o] -> [fo]\n",
      "386: [e][d] -> [ed]\n",
      "387: [�][�] -> [ú]\n",
      "388: [�][�] -> [õ]\n",
      "389: [o][c] -> [oc]\n",
      "390: [o][v] -> [ov]\n",
      "391: [p][ro] -> [pro]\n",
      "392: [p][el] -> [pel]\n",
      "393: [. ][A] -> [. A]\n",
      "394: [b][r] -> [br]\n",
      "395: [an][d] -> [and]\n",
      "396: [-][ ] -> [- ]\n",
      "397: [a][g] -> [ag]\n",
      "398: [p][or ] -> [por ]\n",
      "399: [es][s] -> [ess]\n",
      "400: [fo][i ] -> [foi ]\n",
      "401: [e][c] -> [ec]\n",
      "402: [0][ ] -> [0 ]\n",
      "403: [õ][es ] -> [ões ]\n",
      "404: [20][0] -> [200]\n",
      "405: [f][or] -> [for]\n",
      "406: [o][. ] -> [o. ]\n",
      "407: [r][o ] -> [ro ]\n",
      "408: [n][ci] -> [nci]\n",
      "409: [com][o ] -> [como ]\n",
      "410: [u][s] -> [us]\n",
      "411: [u][b] -> [ub]\n",
      "412: [at][eg] -> [ateg]\n",
      "413: [t][r] -> [tr]\n",
      "414: [O][ ] -> [O ]\n",
      "415: [o][g] -> [og]\n",
      "416: [ateg][or] -> [ategor]\n",
      "417: [c][h] -> [ch]\n",
      "418: [1][ ] -> [1 ]\n",
      "419: [a][p] -> [ap]\n",
      "420: [i][z] -> [iz]\n",
      "421: [ad][os ] -> [ados ]\n",
      "422: [ategor][ia] -> [ategoria]\n",
      "423: [ategoria][:] -> [ategoria:]\n",
      "424: [ç][ão ] -> [ção ]\n",
      "425: [E][st] -> [Est]\n",
      "426: [an][do ] -> [ando ]\n",
      "427: [as][s] -> [ass]\n",
      "428: [á][ri] -> [ári]\n",
      "429: [y][ ] -> [y ]\n",
      "430: [o ][do ] -> [o do ]\n",
      "431: [ic][a ] -> [ica ]\n",
      "432: [a][. ] -> [a. ]\n",
      "433: [i][s ] -> [is ]\n",
      "434: [d][es] -> [des]\n",
      "435: [i][on] -> [ion]\n",
      "436: [ ][(] -> [ (]\n",
      "437: [e][v] -> [ev]\n",
      "438: [am][ent] -> [ament]\n",
      "439: [f][ic] -> [fic]\n",
      "440: [d][as ] -> [das ]\n",
      "441: [I][n] -> [In]\n",
      "442: [in][h] -> [inh]\n",
      "443: [se][u ] -> [seu ]\n",
      "444: [am][b] -> [amb]\n",
      "445: [os][, ] -> [os, ]\n",
      "446: [c][l] -> [cl]\n",
      "447: [a][z] -> [az]\n",
      "448: [er][s] -> [ers]\n",
      "449: [u][s ] -> [us ]\n",
      "450: [;][ ] -> [; ]\n",
      "451: [in][g] -> [ing]\n",
      "452: [m][ais ] -> [mais ]\n",
      "453: [é][m ] -> [ém ]\n",
      "454: [o][t] -> [ot]\n",
      "455: [A][n] -> [An]\n",
      "456: [r][u] -> [ru]\n",
      "457: [�][�] -> [�]\n",
      "458: [20][1] -> [201]\n",
      "459: [ ][e ] -> [ e ]\n",
      "460: [�][�] -> [â]\n",
      "461: [C][ategoria:] -> [Categoria:]\n",
      "462: [p][ri] -> [pri]\n",
      "463: [a ][do ] -> [a do ]\n",
      "464: [U][n] -> [Un]\n",
      "465: [e][p] -> [ep]\n",
      "466: [t][o ] -> [to ]\n",
      "467: [su][a ] -> [sua ]\n",
      "468: [ent][r] -> [entr]\n",
      "469: [u][g] -> [ug]\n",
      "470: [=][ ] -> [= ]\n",
      "471: [e ][de ] -> [e de ]\n",
      "472: [4][ ] -> [4 ]\n",
      "473: [par][t] -> [part]\n",
      "474: [a][o ] -> [ao ]\n",
      "475: [as][, ] -> [as, ]\n",
      "476: [i][o ] -> [io ]\n",
      "477: [l][ic] -> [lic]\n",
      "478: [id][ade ] -> [idade ]\n",
      "479: [a ][e ] -> [a e ]\n",
      "480: [ci][d] -> [cid]\n",
      "481: [h][e ] -> [he ]\n",
      "482: [. ][E] -> [. E]\n",
      "483: [�][�] -> [à]\n",
      "484: [p][e] -> [pe]\n",
      "485: [s][, ] -> [s, ]\n",
      "486: [)][, ] -> [), ]\n",
      "487: [2][ ] -> [2 ]\n",
      "488: [v][i] -> [vi]\n",
      "489: [p][ort] -> [port]\n",
      "490: [ant][e ] -> [ante ]\n",
      "491: [os ][de ] -> [os de ]\n",
      "492: [a ][da ] -> [a da ]\n",
      "493: [c][ul] -> [cul]\n",
      "494: [en][s] -> [ens]\n",
      "495: [as ][de ] -> [as de ]\n",
      "496: [ê][nci] -> [ênci]\n",
      "497: [6][ ] -> [6 ]\n",
      "498: [ar][t] -> [art]\n",
      "499: [r][es] -> [res]\n",
      "500: [in][t] -> [int]\n",
      "501: [1][8] -> [18]\n",
      "502: [5][ ] -> [5 ]\n",
      "503: [gu][es] -> [gues]\n",
      "504: [an][ ] -> [an ]\n",
      "505: [3][ ] -> [3 ]\n",
      "506: [19][8] -> [198]\n",
      "507: [. A][ ] -> [. A ]\n",
      "508: [p][l] -> [pl]\n",
      "509: [m][in] -> [min]\n",
      "510: [ci][on] -> [cion]\n",
      "511: [es][, ] -> [es, ]\n"
     ]
    }
   ],
   "source": [
    "bpe512.save(\"voc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_activities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
